{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7d5297-67ae-4d11-b887-ef8d0b7de416",
   "metadata": {},
   "source": [
    "# The simplest tree\n",
    "\n",
    "Let’s build the simplest tree model we can think of: a classification tree with only one split. Decision trees of this form are commonly referred to under the umbrella term Classification and Regression Trees (CART) [1]. For this initial investigation, we'll be attempting to predict patient mortality using only the `age` and `acutephysiologyscore` features.\n",
    "\n",
    "While we will only be looking at classification here, regression isn’t too different. After grouping the data (which is essentially what a decision tree does), classification involves assigning all members of the group to the majority class of that group during training. Regression is the same, except you would assign the average value, not the majority.\n",
    "\n",
    "In the case of a decision tree with one split, often called a “stump”, the model will partition the data into two groups, and assign classes for those two groups based on majority vote. There are many parameters available for the `DecisionTreeClassifier` class; by specifying `max_depth=1` we will build a decision tree with only one split - i.e. of depth 1.\n",
    "\n",
    "As this is a simple decision tree, we will use the tree module of the `sklearn` Python package. Later on in the workshop we will move onto more advanced techniques from different libraries. \n",
    "\n",
    "[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.\n",
    "\n",
    "Let's begin by loading our processed data and re-creating our train/test splits from the previous workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92257f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mattswatson/intro-to-trees-workshop/refs/heads/main/eicu_processed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c4a2f",
   "metadata": {},
   "source": [
    "We also have a helper function that we'll use to visualise our decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tree_boundaries(model, x_train, y_train, feature_names, target_names):\n",
    "    # Parameters\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    plot_colors = \"rb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    g = DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        x_train,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        xlabel=feature_names[0],\n",
    "        ylabel=feature_names[1],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_train == i)[0]\n",
    "        plt.scatter(\n",
    "            x_train.iloc[idx, 0],\n",
    "            x_train.iloc[idx, 1],\n",
    "            c=color,\n",
    "            label=target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15\n",
    "        )\n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61173572-34f2-45cb-8f5b-5541b9ff41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "# For this study, we only want age and acutephysiologyscore\n",
    "features = ['age', 'acutephysiologyscore']\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state =  42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6612249-e85d-4c0b-b7cb-52a3d45092c8",
   "metadata": {},
   "source": [
    "Create our tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d5c1ef-175f-4828-960c-a9eaf98eab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# specify max_depth=1 so we train a stump, i.e. a tree with only 1 split\n",
    "model = tree.DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# fit the model to the data - trying to predict y from X\n",
    "model = model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe0897-16e5-4b05-8662-02e630fb85b1",
   "metadata": {},
   "source": [
    "Because our tree is a stump, and we only have two predictor variables, we can easily visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936eba3-f894-4e2e-b614-bd2b1ae857b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(model, feature_names=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892522b-d760-4309-8188-7397ea28bee8",
   "metadata": {},
   "source": [
    "Here we see three nodes: a node at the top, a node in the lower left, and a node in the lower right.\n",
    "\n",
    "The top node is the root of the tree: it contains all the data. Let’s read this node bottom to top:\n",
    "\n",
    "- `value = [339, 36]`: Current class balance. There are 339 observations of class 0 and 36 observations of class 1.\n",
    "samples = 375: Number of samples assessed at this node.\n",
    "- `gini = 0.174`: Gini impurity, a measure of “impurity”. The higher the value, the bigger the mix of classes. A 50/50 split of two classes would result in an index of 0.5.\n",
    "- `acutePhysiologyScore <=78.5`: Decision rule learned by the node. In this case, patients with a score of <= 78.5 are moved into the left node and >78.5 to the right.\n",
    "\n",
    "The gini impurity is actually used by the algorithm to determine a split. The model evaluates every feature (in our case, age and score) at every possible split (46, 47, 48..) to find the point with the lowest gini impurity in two resulting nodes.\n",
    "\n",
    "The approach is referred to as “greedy” because we are choosing the optimal split given our current state. Let’s take a closer look at our decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e92f3c-d02a-4013-bd7b-fb045b92b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_tree_boundaries\n",
    "\n",
    "# This is a custom function for this workshop that displays simple decision trees\n",
    "plot_tree_boundaries(model, x_test, y_test, features, ['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513de4c-8155-4167-ac8f-5e2554aa684c",
   "metadata": {},
   "source": [
    "In this plot we can see the decision boundary on the y-axis, separating the predicted classes. The true classes are indicated at each point. Where the background and point colours are mismatched, there has been misclassification. Of course we are using a very simple model.\n",
    "\n",
    "Throughout the few workbooks, we'll be looking at different types of decision trees, all of which are implemented in the `sklearn` Python package. Take some time to familiarise yourself with the [documentation](https://scikit-learn.org/stable/modules/tree.html) - in future workbooks, you'll have the chance to try to implement the models yourself before looking at the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d11923-90bb-4820-8927-c395ef700b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
