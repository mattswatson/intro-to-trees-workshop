{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0794a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mattswatson/intro-to-trees-workshop/refs/heads/main/eicu_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d30a91-5512-4542-a060-379cdbeeb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tree_boundaries(model, x_train, y_train, feature_names, target_names):\n",
    "    # Parameters\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    plot_colors = \"rb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    g = DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        x_train,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        xlabel=feature_names[0],\n",
    "        ylabel=feature_names[1],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_train == i)[0]\n",
    "        plt.scatter(\n",
    "            x_train.iloc[idx, 0],\n",
    "            x_train.iloc[idx, 1],\n",
    "            c=color,\n",
    "            label=target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15\n",
    "        )\n",
    "        \n",
    "    return g\n",
    "\n",
    "features = ['age','acutephysiologyscore']\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8eb5d2-8fa3-4787-b451-06b7d94c37ab",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Next, we move on to gradient boosting. Gradient boosting elegantly combines concepts from the previous methods. As a “boosting” method, gradient boosting involves iteratively building trees, aiming to improve upon misclassifications of the previous tree. Gradient boosting also borrows the concept of sub-sampling the variables (just like Random Forests), which can help to prevent overfitting.\n",
    "\n",
    "While it is too much to express in this tutorial, the biggest innovation in gradient boosting is that it provides a unifying mathematical framework for boosting models. The approach explicitly casts the problem of building a tree as an optimization problem, defining mathematical functions for how well a tree is performing (which we had before) and how complex a tree is. In this light, one can actually treat AdaBoost as a “special case” of gradient boosting, where the loss function is chosen to be the exponential loss.\n",
    "\n",
    "Again, our data cannot contain missing data. Let’s fix that and build a gradient boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9108326b-bd95-49a7-b43d-d1d8863d6ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data with -1\n",
    "data_no_nans = data.fillna(-1)\n",
    "\n",
    "x = data_no_nans[features]\n",
    "y = data_no_nans[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state =  42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f33ea8-1109-48df-b403-bad33fe3808e",
   "metadata": {},
   "source": [
    "**Task:** Use [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) to train a gradient boosting classifier with 10 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e611c70-e330-4dc4-8791-699245566629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "np.random.seed(321)\n",
    "model =\n",
    "model =\n",
    "\n",
    "plot_tree_boundaries(model, x_train, y_train, feature_names=features, target_names=['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7178f594-6915-4759-a848-5f3031656fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
