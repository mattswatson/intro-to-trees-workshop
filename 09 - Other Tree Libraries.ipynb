{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48aecc0-b2af-40b2-ba80-63403cbe28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utils import plot_tree_boundaries\n",
    "\n",
    "features = ['age','acutephysiologyscore']\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b396a-0251-46b2-ae98-14196a2225d8",
   "metadata": {},
   "source": [
    "# LightGBM and xgboost\n",
    "\n",
    "In all of our previous workbooks, we've focused on creating decision trees that use only two features - `age` and `acutephysiologyscore`. While this made it easier to understand how each of the techniques we used results in different models, as we saw in Workbook 08 it doesn't necessarily provide us with the best quality model.\n",
    "\n",
    "Furthermore, so far we've focused on using the `sklearn` library to train our models. Whilst this library is extremely useful - and in many cases is able to create high-performing models - it is always helpful to have other tools in our toolbox. [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) is a framework for creating gradient boosted decision trees with a Python API. [xgboost](https://xgboost.readthedocs.io/en/stable/) is a similar library. Although they can both be used for the same end goal, they each have their own pros and cons.\n",
    "\n",
    "**Question:** What are the differences between LightGBM and xgboost? When might you want to use one over the other?\n",
    "\n",
    "This is an open-ended workbook - the purpose is for you to choose one of either LightGBM or xgboost and use it to create the best performing model for mortality prediction on our dataset that you can. Be sure to use your chosen library's developer documentation to help you, and feel free to use any other resources that you wish; if you want to go the extra mile, perhaps look at methods for [data imputation](https://scikit-learn.org/stable/modules/impute.html), or techniques to handle [imbalanced data](https://imbalanced-learn.org/stable/). Use the techniques from Workbook 08 to evaluate your final models - and be sure to remember what we learned in Workbook 03 - we don't want our models to overfit!\n",
    "\n",
    "**Tip:** If you want to install a new Python package, you can do so in a Jupyter Notebook code cell with the following command: `!pip install package-name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337e588-2916-4772-90ee-a1f738007ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
