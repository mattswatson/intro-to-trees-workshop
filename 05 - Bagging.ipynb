{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mattswatson/intro-to-trees-workshop/refs/heads/main/eicu_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e090c0-6f8a-43b4-8267-b001fb9352ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tree_boundaries(model, x_train, y_train, feature_names, target_names):\n",
    "    # Parameters\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    plot_colors = \"rb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    g = DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        x_train,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        xlabel=feature_names[0],\n",
    "        ylabel=feature_names[1],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_train == i)[0]\n",
    "        plt.scatter(\n",
    "            x_train.iloc[idx, 0],\n",
    "            x_train.iloc[idx, 1],\n",
    "            c=color,\n",
    "            label=target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15\n",
    "        )\n",
    "        \n",
    "    return g\n",
    "\n",
    "features = ['age','acutephysiologyscore']\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state =  42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d4e77-a28a-4a77-925d-8132ada33736",
   "metadata": {},
   "source": [
    "In the previous workbook we saw how one ensembling technique, and how it works by reweighting incorrectly classified samples, such that the ensemble's sub-models place more importance on them.\n",
    "\n",
    "Bootstrap aggregation, or “Bagging”, is another form of ensemble learning.\n",
    "\n",
    "With boosting, we iteratively changed the dataset to have new trees focus on the “difficult” observations. Bagging involves the same approach, except we don’t selectively choose which observations to focus on, but rather we randomly select subsets of data each time.\n",
    "\n",
    "Boosting aimed to iteratively improve our overall model with new trees. With bagging, we now build trees on what we hope are independent datasets.\n",
    "\n",
    "Let’s take a step back, and think about a practical example. Say we wanted a good model of heart disease. If we saw researchers build a model from a dataset of patients from their hospital, we might think this would be sufficient. If the researchers were able to acquire a new dataset from new patients, and built a new model, we’d be inclined to feel that the combination of the two models would be better than any one individually.\n",
    "\n",
    "This is the scenario that bagging aims to replicate, except instead of actually going out and collecting new datasets, we instead use “bootstrapping” to create new sets of data from our current dataset. If you are unfamiliar with bootstrapping, you can treat it as magic for now (and if you are familiar with the bootstrap, you already know that it is magic).\n",
    "\n",
    "Let’s take a look at a simple bootstrap model.\n",
    "\n",
    "**Task:** Use [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) to train a bagging decision tree with 6 estimators, where the `base_estimator` is a `DecisionTreeClassifier` with `max_depth=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9833b-a0bc-4ab6-baee-58c2ada0ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree, ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(321)\n",
    "tree_model = \n",
    "ensemble_model = \n",
    "ensemble_model = ensemble_model.fit(x_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=[12,6])\n",
    "for i, estimator in enumerate(ensemble_model.estimators_):    \n",
    "    plot_tree_boundaries(estimator, x_train, y_train, feature_names=features, target_names=['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6019a8-8dc6-47d8-a47f-7007e4d6d385",
   "metadata": {},
   "source": [
    "We can see that each individual tree varies considerably. This is a result of using a random set of data to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ceb1ba-0fe1-4538-8f96-3979c373eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_boundaries(ensemble_model, x_train, y_train, feature_names=features,  target_names=['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80007aee-6de3-4464-a160-ec9a7993b70b",
   "metadata": {},
   "source": [
    "Of course, since this is a simple dataset, we are not seeing that many dramatic changes between different models. Don’t worry, we’ll quantitatively evaluate them later.\n",
    "\n",
    "Next up, a minor addition creates one of the most popular models in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7429c6-7a28-4062-af11-606adeb3783b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
