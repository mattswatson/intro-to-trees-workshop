{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e266a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mattswatson/intro-to-trees-workshop/refs/heads/main/eicu_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45139707-9bb9-47de-b693-5ff0b11e96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tree_boundaries(model, x_train, y_train, feature_names, target_names):\n",
    "    # Parameters\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    plot_colors = \"rb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    g = DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        x_train,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        xlabel=feature_names[0],\n",
    "        ylabel=feature_names[1],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_train == i)[0]\n",
    "        plt.scatter(\n",
    "            x_train.iloc[idx, 0],\n",
    "            x_train.iloc[idx, 1],\n",
    "            c=color,\n",
    "            label=target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15\n",
    "        )\n",
    "        \n",
    "    return g\n",
    "\n",
    "features = ['age','acutephysiologyscore']\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8578712-016c-41da-b3f2-f95d6568ec41",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "In the previous example, we used bagging to randomly resample our data to generate “new” datasets. The Random Forest takes this one step further: instead of just resampling our data, we also select only a fraction of the features to include.\n",
    "\n",
    "It turns out that this subselection tends to improve the performance of our models. The odds of an individual being very good or very bad is higher (i.e. the variance of the trees is increased), and this ends up giving us a final model with better overall performance (lower bias).\n",
    "\n",
    "Much like the boosting technique from earlier, the `sklearn` implementation of random forests do not support missing data. Let's replace all missing data with -1, then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f004e66-5f2d-406a-b391-08521c27f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data with -1\n",
    "data_no_nans = data.fillna(-1)\n",
    "\n",
    "x = data_no_nans[features]\n",
    "y = data_no_nans[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state =  42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6625f6-12c6-413e-b1cf-baedd678ac56",
   "metadata": {},
   "source": [
    "**Task:** Use [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to train a random forest with 6 estimators, max depth of 5 and max features of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba563f-ed56-43f5-9334-2ac2434953b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(321)\n",
    "model = \n",
    "model = \n",
    "\n",
    "for i, estimator in enumerate(model.estimators_):    \n",
    "    plot_tree_boundaries(estimator, x_train, y_train, feature_names=features, target_names=['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78daee-d56f-45ad-9533-90a01cd73031",
   "metadata": {},
   "source": [
    "Let's also look at the overall final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16146a02-91d0-432f-8bef-fd2328ff70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_boundaries(model, x_train, y_train, feature_names=features, target_names=['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc3338-ad38-4a9e-8e3c-bf933e210c2e",
   "metadata": {},
   "source": [
    "Much like the bagging model, this ensembling technique creates a much more complex (and possibly accurate - we'll investigate this later) decision boundary. Random forests are a powerful technique that can be used to achieve state of the art results on some tasks - in particualr, those that involve tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d863d-fb9f-42f0-b8ea-0272ce4460a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
