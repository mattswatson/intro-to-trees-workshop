{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mattswatson/intro-to-trees-workshop/refs/heads/main/eicu_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tree_boundaries(model, x_train, y_train, feature_names, target_names):\n",
    "    # Parameters\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    plot_colors = \"rb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    g = DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        x_train,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        xlabel=feature_names[0],\n",
    "        ylabel=feature_names[1],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_train == i)[0]\n",
    "        plt.scatter(\n",
    "            x_train.iloc[idx, 0],\n",
    "            x_train.iloc[idx, 1],\n",
    "            c=color,\n",
    "            label=target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15\n",
    "        )\n",
    "        \n",
    "    return g\n",
    "\n",
    "features = ['age','acutephysiologyscore']\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state =  42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger Depth Trees\n",
    "\n",
    "In the previous episode we created a very simple decision tree. Let’s see what happens when we introduce new decision points by increasing the depth.\n",
    "\n",
    "**Task:** Using the template code from the previous workbook, and the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), train a decision tree with `max_depth` of 5. Plot the decision tree using `plot_tree_boundaries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train model\n",
    "\n",
    "# plot tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our tree is more complicated! We can see a few vertical boundaries as well as the horizontal one from before. Some of these we may like, but some appear unnatural. Let’s look at the tree itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(24, 24))\n",
    "tree.plot_tree(model, filled=True, feature_names=features, class_names=['Alive', 'Dead'], fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the tree, we can see that there are some very specific sets of rules - e.g., there are some leaves with only 1 or 2 samples.\n",
    "\n",
    "**Question:** Consider a patient aged 45 years with an acute physiology score of 100. Using the image of the tree, work through the nodes until your can make a prediction. What outcome does your model predict?\n",
    "\n",
    "**Question:** What is the gini impurity of the final node, and why?\n",
    "\n",
    "**Question:** Does the decision that led to this final node seem sensible Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specifcity shows that our model is overfitting to the training data. Overfitting is a problem that occurs when our algorithm is too closely aligned to our training data, and often occurs when we use a powerful technique to model simple data.\n",
    "\n",
    "The result is that the model may not generalise well to “unseen” data, such as observations for new patients entering a critical care unit - the model knows what to do in settings that closely follow the training data, but has no knowledge of data outside of this very narrow distribution. \n",
    "\n",
    "There are techniques we can employ that try to address this problem. This is where “pruning” comes in.\n",
    "\n",
    "## Pruning\n",
    "\n",
    "Let's run pruning, run the model again and see what it does.\n",
    "\n",
    "**Task:** Look at the sklearn documentation, and train a decision tree with `max_depth` and `min_samples_leaf` of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "\n",
    "# Plot tree\n",
    "plt.clf()\n",
    "plt.figure(figsize=(24, 24))\n",
    "tree.plot_tree(model, filled=True, feature_names=features, class_names=['Alive', 'Dead'], fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compare the new tree to the previous one - what's different?\n",
    "\n",
    "We can see that our second tree is (1) smaller in depth, and (2) never splits a node with <= 10 samples. We can look at the decision surface for this tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_boundaries(model, x_train, y_train, features, ['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pruned decision tree has a more intuitive boundary, but does make some errors. We have reduced our performance in an effort to simplify the tree. This is the classic machine learning problem of trading off complexity with error.\n",
    "\n",
    "Note that, in order to do this, we “invented” the minimum samples per leaf node of 10. Why 10? Why not 5? Why not 20? The answer is: it depends on the dataset. Heuristically choosing these parameters can be time consuming, and we will see later on how gradient boosting elegantly handles this task.\n",
    "\n",
    "## Decisions trees have \"high variance\"\n",
    "\n",
    "Decision trees have high “variance”. In this context, variance refers to a property of some models to have a wide range of performance given random samples of data. Let’s take a look at randomly slicing the data we have to see what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "fig = plt.figure(figsize=[12,3])\n",
    "\n",
    "for i in range(3):\n",
    "    # generate indices in a random order\n",
    "    idx = np.random.permutation(x_train.shape[0])\n",
    "    \n",
    "    # only use the first 50\n",
    "    idx = idx[:50]\n",
    "    x_temp = x_train.iloc[idx]\n",
    "    y_temp = y_train.values[idx]\n",
    "    \n",
    "    # initialize the model\n",
    "    model = tree.DecisionTreeClassifier(max_depth=5)\n",
    "    \n",
    "    # train the model using the dataset\n",
    "    model = model.fit(x_temp.values, y_temp)\n",
    "    plot_tree_boundaries(model, x_temp, y_temp, features, ['Alive', 'Dead'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that we are using random subsets of data, and as a result, our decision boundary can change quite a bit. As you could guess, we actually don’t want a model that randomly works well and randomly works poorly.\n",
    "\n",
    "There is an old joke: two farmers and a statistician go hunting. They see a deer: the first farmer shoots, and misses to the left. The next farmer shoots, and misses to the right. The statistician yells “We got it!!”.\n",
    "\n",
    "While it doesn’t quite hold in real life, it turns out that this principle does hold for decision trees. Combining them in the right way ends up building powerful models.\n",
    "\n",
    "**Question:** Why are decision trees considered have high variance?\n",
    "\n",
    "**Question:** An “ensemble” is the name used for a machine learning model that aggregates the decisions of multiple sub-models. Why might creating ensembles of decision trees be a good idea?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
