{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761aa550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mattswatson/intro-to-trees-workshop/refs/heads/main/eicu_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48aecc0-b2af-40b2-ba80-63403cbe28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tree_boundaries(model, x_train, y_train, feature_names, target_names):\n",
    "    # Parameters\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    plot_colors = \"rb\"\n",
    "    plot_step = 0.02\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    g = DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        x_train,\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        xlabel=feature_names[0],\n",
    "        ylabel=feature_names[1],\n",
    "    )\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_train == i)[0]\n",
    "        plt.scatter(\n",
    "            x_train.iloc[idx, 0],\n",
    "            x_train.iloc[idx, 1],\n",
    "            c=color,\n",
    "            label=target_names[i],\n",
    "            cmap=plt.cm.RdYlBu,\n",
    "            edgecolor=\"black\",\n",
    "            s=15\n",
    "        )\n",
    "        \n",
    "    return g\n",
    "\n",
    "features = ['age','acutephysiologyscore']\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b396a-0251-46b2-ae98-14196a2225d8",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "We’ve now learned the basics of the various tree methods and have visualized most of them; however, how do we actually know which one is the best predictive model? Let’s finish by comparing the performance of our models on our held-out test data. Our goal, remember, is to predict whether or not a patient will survive their hospital stay using the patient’s age and acute physiology score computed on the first day of their ICU stay.\n",
    "\n",
    "We will begin by training a model for each of the techniques we have looked at so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0faa4df5-6001-490f-88db-a965b222a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data with -1\n",
    "data_no_nans = data.fillna(-1)\n",
    "\n",
    "x = data_no_nans[features]\n",
    "y = data_no_nans[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7, random_state =  42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c35baf-f1e1-45c0-ac53-857b6eaa0523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, ensemble, tree\n",
    "\n",
    "models = dict()\n",
    "models['Decision Tree'] = tree.DecisionTreeClassifier(criterion='entropy', splitter='best').fit(x_train, y_train)\n",
    "models['Gradient Boosting'] = ensemble.GradientBoostingClassifier(n_estimators=10).fit(x_train, y_train)\n",
    "models['Random Forest'] = ensemble.RandomForestClassifier(n_estimators=10).fit(x_train, y_train)\n",
    "models['Bagging'] =  ensemble.BaggingClassifier(n_estimators=10).fit(x_train, y_train)\n",
    "models['AdaBoost'] =  ensemble.AdaBoostClassifier(n_estimators=10).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f9940-afb0-44fc-a95c-cf2b9ea5239f",
   "metadata": {},
   "source": [
    "We now have a model for each of the techniques we have looked at. There are a number of different metrics we can use to measure the performance of our model, depending on what we want to measure. The most basic is accuracy - on the test set, how many predictions does our model get correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf3618-69ef-49be-94b8-77dd01e8d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy\\tModel')\n",
    "for current_model in models:    \n",
    "    predicted_proba = models[current_model].predict_proba(x_test)[:, 1]\n",
    "    predictions = models[current_model].predict(x_test)\n",
    "    \n",
    "    score = metrics.accuracy_score(y_test, predictions)\n",
    "    print('{:0.3f}\\t{}'.format(score, current_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004c650-b087-4388-97b0-09834f5d5c29",
   "metadata": {},
   "source": [
    "As you might expect (but is, crucially, not always the case!), the more advanced gradient boosting technique achieves the best performance.\n",
    "\n",
    "**Question:** What is a possible issue with relying on accuracy as a performance metric?\n",
    "\n",
    "Accuacy is not always a suitable performance metric, especially when our data is unbalanced (i.e. there are many more samples in one class than the other).\n",
    "\n",
    "Another issue with accuracy is that it only measures performance at a single decision threshold. To inspect how well our models perform at different thresholds, we can plot the Receiver Operating Characteristic curve. This curve plots the false positive rate (x-axis) against the true positive rate (y-axis) at different decision thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives (and vice-versa when increasing the threshold). We can summarise this curve by calculating the area under the curve (AUROC, for Area Under the Receiver Operating Characteristic curve).\n",
    "\n",
    "**Task:** Search for the correct functions in [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to display the ROC curve, and calculate the AUROC for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6d0b1-6102-412c-bfe1-b977d7145c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "for current_model in models:    \n",
    "   # Display the ROC curve\n",
    "\n",
    "print('AUROC\\tModel')\n",
    "for current_model in models:    \n",
    "    # Calculate and print AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545aaf4-4f7a-494b-9adc-bf846189d5c7",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "A binary classification model can fail in one of two ways: false negatives (a predictive model misses a positive sample) and false positives (a predictive model incorrectly labels a negative sample as positive). In some cases, we may prefer our model to make one type of error over the other. For example, in our mortality prediction scenario, it may be desireable to make fewer false negative classifications (as this means we will miss patients at risk of critical deterioration) at the expense of making more false positive errors. Conversely, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives).\n",
    "\n",
    "This is where metrics such as precision and recall come in. Precision measures: What proportion of positive identifications was actually correct?\n",
    "\n",
    "**Question:** Can you come up with a possible formula for precision?\n",
    "\n",
    "Recall, on the other hand, measures proportion of actual positives was identified correctly?\n",
    "\n",
    "**Question:** Can you come up with a possible formula for recall?\n",
    "\n",
    "Let's calculate the precision and recall of our models.\n",
    "\n",
    "**Task:** Search for the correct functions in [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) to calculate the precision and recall for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbddd2a-966b-4a07-8ec9-8ad77b752f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision\\tRecall\\tModel')\n",
    "for current_model in models:    \n",
    "    # Calculate and print the precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0f74e-5ed8-47fd-b50d-a770bd3e984f",
   "metadata": {},
   "source": [
    "We can see that all of our models have low precision and recall scores, despite having high AUROC and accuracy. Let's take a look at the Gradient Boosting results in a bit more detail to see what these results are telling us.\n",
    "\n",
    "**Question:** The gradient boosting model has a precision of 0.6667 - what does this mean in terms of the positively classified predictions?\n",
    "\n",
    "**Question:** The gradient boosting model has a recall of 0.333 - what does this mean in terms of our predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced3c23-308e-41f5-903c-982ccbd7db19",
   "metadata": {},
   "source": [
    "As our model has a precision of 0.6667, this means that when it predicts a patient will not survive their hospital stay, it is correct two thirds of the time. With a recall of 0.3333, it means we correctly 33% of patients who do not survive their stay.\n",
    "\n",
    "Precision and recall are opposing metrics - by increasing the decision threshold we will increase precision while reducing recall, and vice-versa. We can visualise this by plotting a precision recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adc363-239a-4e13-b729-e9e8b35af723",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "for current_model in models:    \n",
    "    # Show the precision-recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca7247-1ca3-4046-a9f9-62cfc6f8f63d",
   "metadata": {},
   "source": [
    "The F-score measures this trade-off between precision and recall, and is often used to give a general idea of model performance - especially when the data is imbalanced. Typically we use the F1 score, which is the harmonic mean of precision and recall, but this can be changed to the F-β score, which weights precision by β.\n",
    "\n",
    "**Question:** What is the formula for the F-β score?\n",
    "\n",
    "Let's calculate the F1 score of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf318500-8c2d-4c49-8efc-9ac4fa4d06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1\\tModel')\n",
    "for current_model in models:    \n",
    "    # Calculate the F1 score for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae72f0-741d-4b17-a885-b86e8e0b7499",
   "metadata": {},
   "source": [
    "With such low precision and recall values, our models may not be very useful in practice. There are multiple possible explanations for this: our task might be difficult, our models might not be powerful enough and so on. One notable point is that our data is imbalanced, and we have (so far) only used two features (age and acute physiology score) in our models.\n",
    "\n",
    "**Question:** What proportion of our data is classified as a positive class (i.e. how many patients do not survive their hospital stay)?\n",
    "\n",
    "In the next (and final) workbook, we will look at using different Python libraries to create more complex models that incorporate all available features in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
